{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Model Live Testing Notebook.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"38ce0b9e"},"source":["###### FileName :  Model Live Testing\n","###### Date     : March 23, 2021\n","###### Author   : Muhammad Hassaan Bashir"],"id":"38ce0b9e"},{"cell_type":"markdown","metadata":{"id":"30530065"},"source":["## Importing Required Libraries"],"id":"30530065"},{"cell_type":"code","metadata":{"id":"a26f4d84"},"source":["# ==================================\n","# Importing libraries\n","# ==================================\n","\n","# For video streaming and face detection  \n","import cv2\n","# operating system code library for file open and creation\n","import os\n","# using to make arrays anfd apply mathematical expressions\n","import numpy as np\n","# using for plotting images on graph\n","import matplotlib.pyplot as plt\n","# using for convolution technique \n","import tensorflow as tf \n","from tensorflow import keras"],"id":"a26f4d84","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"818b0dcb"},"source":["## Loading Haar-Cascade Face Detection Class from OPEN-CV"],"id":"818b0dcb"},{"cell_type":"code","metadata":{"id":"e9b05f07"},"source":["# ======================================\n","# HaarCascade Face Detectoin class\n","# ======================================\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n","# images in our dataset are classified in 7 categories\n","label_to_text = { 0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4:'sadness', 5:'surprise', 6:'neutral' }"],"id":"e9b05f07","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3be793ad"},"source":["## Parameters and Other Config Variables"],"id":"3be793ad"},{"cell_type":"code","metadata":{"id":"3fab49c2"},"source":["# ======================================\n","# parameters passed \n","# ======================================\n","img_size = 48   # img resize as per the input size of model\n","num_classes = 7 # categories of classes"],"id":"3fab49c2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95d94ae8"},"source":["## Setting Font Configuration that will be visible on video Frame"],"id":"95d94ae8"},{"cell_type":"code","metadata":{"id":"d8e99ee6"},"source":["# setting font \n","font = cv2.FONT_HERSHEY_SIMPLEX\n","  \n","# fontScale\n","fontScale = 1\n","   \n","# Blue color in BGR\n","color = (255, 0, 0)\n","  \n","# Line thickness of 2 px\n","thickness = 2"],"id":"d8e99ee6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e118cd73"},"source":["## Initialize Camera Module"],"id":"e118cd73"},{"cell_type":"code","metadata":{"id":"3a6b89f1"},"source":["# Capturing video from webcam\n","video_capture = cv2.VideoCapture(0)"],"id":"3a6b89f1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4c27c93"},"source":["## Loading Trained Model"],"id":"f4c27c93"},{"cell_type":"code","metadata":{"id":"545cb77f"},"source":["# Loading trained model \n","basemodel = keras.models.load_model('base_model.h5')"],"id":"545cb77f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aa52642f"},"source":["## Main Loop Continuous Frames to Get the Frame by Frame image and pass it to model for detection"],"id":"aa52642f"},{"cell_type":"code","metadata":{"id":"b99dbc9b","outputId":"fd72acee-bcef-4278-b730-7585d59829c7"},"source":["# ======================================\n","# For getting images continously\n","# ======================================\n","while True:\n","    # Capture frame-by-frame\n","    ret, frame = video_capture.read()\n","    # The Model is trained on grayscale dataset so we need to convert the frame in grayscale image\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # detecting mutliscale images\n","    faces = face_cascade.detectMultiScale(\n","                                            gray,\n","                                            scaleFactor=1.3, # fo scaling of image \n","                                            minNeighbors=3,  # minimum gap in images\n","                                            minSize=(30, 30) # minimum size of image to be detected\n","                                        )\n","\n","    # Draw a rectangle around the faces\n","    for (x, y, w, h) in faces:\n","        # rectangle dimensions\n","        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n","        # Get Region of Interest Frame (Means get the face area of image)\n","        roi_color = frame[y:y + h, x:x + w]\n","        # Convert RGB image to Grayscale\n","        roi_color_gray = cv2.cvtColor(roi_color, cv2.COLOR_BGR2GRAY)\n","        # Resize Image to required img_size i.e. 48x48 as model expect input of this size\n","        roi_color_gray = cv2.resize(roi_color_gray, (img_size,img_size), interpolation = cv2.INTER_AREA)\n","        # Pass the image to model for prediction and get the key number of highest probability using argmax function at end\n","        prediction = basemodel.predict(tf.expand_dims(roi_color_gray,0)).argmax()\n","        # Print the predicted result\n","        #print(label_to_text[prediction])\n","        # Add emotion label as text on image if face is detected with some emotion\n","        cv2.putText(frame, str(label_to_text[prediction]), (x,y) , font, fontScale, color, thickness, cv2.LINE_AA)\n","    # Display the resulting frame\n","    cv2.imshow('Video', frame)\n","    # break the streaming on pressing 'q'\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break"],"id":"b99dbc9b","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["anger\n","neutral\n","anger\n","neutral\n","neutral\n","anger\n","anger\n","neutral\n","anger\n","anger\n","anger\n","neutral\n","neutral\n","neutral\n","neutral\n","happiness\n","anger\n","neutral\n","fear\n","anger\n","neutral\n","neutral\n","neutral\n","surprise\n","neutral\n","neutral\n","neutral\n","neutral\n","fear\n","anger\n","neutral\n","surprise\n","neutral\n","anger\n","neutral\n","anger\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","anger\n","neutral\n","anger\n","anger\n","anger\n","neutral\n","anger\n","neutral\n","anger\n","anger\n","anger\n","neutral\n","neutral\n","happiness\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","neutral\n","happiness\n","neutral\n"]}]}]}